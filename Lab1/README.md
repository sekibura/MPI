# Отчет о выполнении лабораторной работы №1
## Передача и прием сообщений в MPI
### Задание
#### В соответствии с вариантом задания, написать на C++ программу, реализующую многопоточность на основе технологии MPI, работающую на основе программа должна работать на основе простой передачи сообщений.

Реализуйте функцию star, которая создаёт N+1 процессов (1 «центральный» и N «крайних») и посылает сообщение центральному процессу, который посылает сообщение всем остальным процессам и дожидается от них ответа, после чего это повторяется (всего M раз). После того, как все события получены, все процессы заканчивают работу.

[Код программы](https://github.com/sekibura/MPI/blob/main/Lab1/Source.cpp)
#### Работа программы:
![image](https://user-images.githubusercontent.com/51335422/108347680-d9cbc800-71f1-11eb-9072-ad41f0986fca.png)

На скриншоте видно ответы от процессов, которые получил главный процесс при M=1(количество повторений процесса *диалога*) и количестве процессов = 4 (считая главный).

Это свидетельствует о том, что при работе программы произошел требуемый алгоритм действий: главный процесс послал остальным процессам сообщение, те его получили и отправили ответ.

## Котрольные вопросы

1.	В чем состоят основы технологии MPI? 

    Основу MPI составляют операции передачи сообщений.    
    
2.	В чем состоят основные преимущества и недостатки технологии MPI?

    Преимущества
    - MPI помогает решить проблему переносимости параллельных программ между разными компьютерными системами.
    - MPI содействует повышению эффективности параллельных вычислений: практически для каждого типа вычислительных систем существуют реализации библиотек MPI, учитывающие возможности используемого коммуникационного оборудования.
    - MPI облегчает процесс написания параллельных программ, когда при разработке используются библиотеки программных модулей, написанных с использованием этого интерфейса.
    - MPI позволяет создавать хорошо масштабируемые параллельные программы.
    
    Недостатки
    - MPI является низкоуровневым инструментом программиста.
    - Не существует реализаций MPI, в полной мере обеспечивающих совмещение обменов с вычислениями.
    - MPI не предоставляет механизмов задания начального размещения процессов по процессорам.
    - Полномасштабная отладка MPI-программ затруднительна вследствие одновременного исполнения нескольких программных ветвей.
    
3.	Что понимается под параллельной программой в рамках технологии MPI?

    Под параллельной программой в рамках MPI понимается множество одновременно выполняемых процессов.
    Процессы могут выполняться как на разных процессорах, так и на одном. Каждый процесс параллельной программы порождается на основе копии одного и 
    того же программного кода (модель SPMD ). Все процессы программы последовательно перенумерованы от 0 до p-1, где p есть общее количество процессов.
    Номер процесса именуется рангом процесса.
    
4.	Как происходит инициализация и завершение MPI программ?

    Первой вызываемой функцией MPI должна быть функция:
    
    ```
    int MPI_Init(int *agrc, char ***argv);
    ```
    для инициализации среды выполнения MPI-программы. Параметрами функции являются количество аргументов в командной строке и текст самой командной строки.

    
5.	Как происходит передача и прием сообщений MPI программе?

    Для передачи сообщения процесс-отправитель должен выполнить функцию:
    
    ```
    int MPI_Send(void *buf, int count, MPI_Datatype type, int dest,
             int tag, MPI_Comm comm);
    ```
    
    
    Для приема сообщения процесс-получатель должен выполнить функцию:

    ```
    int MPI_Recv(void *buf, int count, MPI_Datatype type, int source,
             int tag, MPI_Comm comm, MPI_Status *status);
    ```
