# Отчет о выполнении лабораторной работы №2
## Коллективные операции передачи данных
### Задание
#### Модифицировать программу, написанную на Л.Р. №1, так чтобы она работала на основе коллективной передачи сообщений. Результаты работы сравнить и занести в отчет.

[Код программы](https://github.com/sekibura/MPI/blob/main/Lab2/Source.cpp)
#### Работа программы:
![image](https://user-images.githubusercontent.com/51335422/108606546-b0ff2a80-73cb-11eb-9c85-74fced6b5e29.png)

На скриншоте видно, что каждый процесс отчитался о получении сообщения от главного процесса.
Также, в конечном итоге, главный процесс получил все ответы.

## Котрольные вопросы

1.	Как происходит передача данных от одного процесса всем? 

    Достижение эффективного выполнения операции передачи данных от одного процесса всем процессам программы (широковещательная рассылка данных) может быть обеспечено при помощи функции MPI:
    ```
    int MPI_Bcast(void *buf,int count,MPI_Datatype type,int root,MPI_Comm comm);
    ```
    Функция MPI_Bcast осуществляет рассылку данных из буфера buf, содержащего count элементов типа type с процесса, имеющего номер root, всем процессам, входящим в коммуникатор   comm.

3.	Как происходит передача данных от всех процессов одному? 

    Передача данных от всех процессов одному происходит с помощью функции:
    ```
    int MPI_Reduce(void *sendbuf, void *recvbuf,int count,MPI_Datatype type,
               MPI_Op op,int root,MPI_Comm comm);

    ```

5.	Какие используются в MPI для синхронизации вычислений?

    Синхронизация процессов, т.е. одновременное достижение процессами тех или иных точек процесса вычислений, обеспечивается при помощи функции MPI:
    ```
    int MPI_Barrier(MPI_Comm comm);
    ```
    При вызове функции MPI_Barrier выполнение процесса блокируется, продолжение вычислений процесса происходит только после вызова функции MPI_Barrier всеми процессами коммуникатора.
    

7.	Как организуется неблокирующий обмен данными между процессами?

    MPI обеспечивает возможность неблокированного выполнения операций передачи данных между двумя процессами. Наименование неблокирующих аналогов образуется из названий соответствующих функций путем добавления префикса I (Immediate). 
    
9.	Как организуется одновременное выполнение прием и передачи данных?

    Достижение эффективного и гарантированного одновременного выполнения операций передачи и приема данных может быть обеспечено при помощи функции MPI:
    ```
    int MPI_Sendrecv(void *sbuf,int scount,MPI_Datatype stype,int dest, int stag,
      void *rbuf,int rcount,MPI_Datatype rtype,int source,int rtag,
      MPI_Comm comm, MPI_Status *status);
    ```
    
    Функция MPI_Sendrecv передает сообщение, описываемое параметрами (sbuf, scount, stype, dest, stag), процессу с рангом dest и принимает сообщение в буфер, определяемый параметрами (rbuf, rcount, rtype, source, rtag), от процесса с рангом source.
В функции MPI_Sendrecv для передачи и приема сообщений применяются разные буфера. В случае же, когда сообщения имеют одинаковый тип, в MPI имеется возможность использования единого буфера.

